# ğŸŒŸ Daily Trending Repositories

<div align="center">
<a href="https://github.com/marc-ko/daily-trending-repo/commits/main">
    <img src="https://img.shields.io/github/last-commit/marc-ko/daily-trending-repo" alt="GitHub last commit" />
</a>

<a href="https://github.com/marc-ko/daily-trending-repo/stargazers">
    <img src="https://img.shields.io/github/stars/marc-ko/daily-trending-repo" alt="GitHub stars" />
</a>
<a href="https://github.com/marc-ko/daily-trending-repo/network/members">
    <img src="https://img.shields.io/github/forks/marc-ko/daily-trending-repo" alt="GitHub forks" />
</a>
<a href="https://github.com/marc-ko/daily-trending-repo/issues">
    <img src="https://img.shields.io/github/issues/marc-ko/daily-trending-repo" alt="GitHub issues" />
</a>
</div>

## ğŸ“‹ About

This project automatically tracks and curates trending repositories from GitHub daily. Stay updated with the most exciting new projects in the developer community! With AI Summarization, you can get the summary of the repository by seeing the README.md file as well!.

### ğŸ”¥ Features

- ğŸ”„ **Weekly Updates**: Fresh content every week Wednesday
- ğŸŒ **Diverse Categories**: Covering all major programming languages and topics
- â­ **Star-based Ranking**: Sorted by community popularity
- ğŸ“Š **Detailed Information**: Including descriptions, languages, and statistics

## ğŸ“ˆ Latest Trending Repositories

Last update: 2025-03-22

<details>
<summary>â„¹ï¸ How to Use This Repository</summary>

1. **Star & Watch**: Click the 'Star' and 'Watch' buttons to receive weekly email notifications
2. **Browse**: Explore trending repositories organized by popularity
3. **Contribute**: Feel free to open issues or suggest improvements

</details>

---

| **Title** | **Description** | **Language** | **Summary** | **Tags** | **Stars Count** | **HTML URL** |
| --- | --- | --- | --- | --- | --- | --- |
| **[rust-stakeholder](https://github.com/giacomo-b/rust-stakeholder)** | Generate impressive-looking terminal output to look busy when stakeholders walk by | Rust | # rust-stakeholder

![Career Status](https://img.shields.io/badge/career-saved-success) ![Stakeholders](https://img.shields.io/badge/stakeholders-impressed-yellow)

**SATIRE ALERT**: This is a joke project poking fun at impostor syndrome and workplace dynamics in tech. No actual deception recommended!

<p align="center">
  <img src="./assets/demo.gif" alt="animated" />
</p>

## Become an irreplaceable 10x developer in 30 seconds flat

Why learn actual skills when you can just *look* impressive instead?

Introducing **rust-stakeholder** - a CLI tool that generates absolutely meaningless but impressive-looking terminal output to convince everyone you're a coding genius without writing a single line of useful code.

> "After using rust-stakeholder, I'm no longer asked about my deadlines but rather for my insights during board meetings." - Developer who still hasn't completed their tickets from last sprint

Remember, it's not about your actual contribution to the codebase; it's about how complicated your terminal looks when the VP of Engineering walks by. Nothing says "I'm vital to this company" like 15 progress bars, cryptic error messages you seem unfazed by, and technical jargon nobody understands.

## Features that add zero value but look incredibly important

- ğŸ–¥ï¸ **Dazzling development simulations**: Make it look like you're solving CERN-level computing problems when you're actually just refreshing Reddit.
- ğŸ§  **Meaningless jargon generator**: Impress with phrases like "Implemented non-euclidean topology optimization for multi-dimensional data representation" (no, it doesn't mean anything).
- ğŸ“Š **Convincing progress bars**: Nothing says "I'm working" like a progress bar slowly advancing while you're in the break room.
- ğŸŒ **Fake network activity**: Simulate mission-critical API requests that are actually just your computer talking to itself.
- ğŸš¨ **Artificial crisis mode**: Generate realistic-looking alerts so people think you're heroically averting disasters.
- ğŸ‘¥ **Imaginary team activity**: Pretend your invisible friends are sending you important pull requests.
- ğŸ® **Domain chameleon**: Switch between backend, frontend, blockchain, and 7 other domains faster than you can say "full-stack developer."

## Installation

```bash
cargo install --git https://github.com/giacomo-b/rust-stakeholder.git
```

Or build from source (warning: might involve actual programming):

```bash
git clone https://github.com/giacomo-b/rust-stakeholder.git
cd rust-stakeholder
cargo build --release # Look at you doing real developer things!
```

## Docker

Build image:

```bash
docker build -t rust-stakeholder .
```

Usage:

Basic usage:

```bash
docker run -t --rm rust-stakeholder
```

All commands below can be used through:

```bash
docker run -t --rm rust-stakeholder [arguments]
```

## Usage for career advancement

Basic usage (for entry-level imposters):

```bash
rust-stakeholder
```

Advanced usage (for senior imposters):

```bash
# Impress the blockchain VC investors
rust-stakeholder --dev-type blockchain --jargon extreme --alerts

# Look busy during performance review season
rust-stakeholder --complexity extreme --team --duration 1800

# Convince everyone you're a 10x game developer
rust-stakeholder --dev-type game-development --framework "Custom Engine" --jargon high

# For the data science frauds
rust-stakeholder --dev-type data-science --jargon extreme --project "Neural-Quantum-Blockchain-AI"

# Emergency mode: Your project is due tomorrow and you haven't started
rust-stakeholder --dev-type fullstack --complexity extreme --alerts --team
```

## Benefits

- **Promotion fast-track**: Skip the tedious "delivering value" step entirely.
- **Meeting domination**: Let it run in the background during calls to seem busy.
- **Deadline extensions**: "Sorry, still resolving those critical system alerts."
- **Salary negotiation**: Just leave it running during your review.
- **Job security**: Become the only person who seems to understand your fictional systems.

## Testimonials

> "I left rust-stakeholder running over the weekend. When I came back on Monday, I had been promoted to Principal Engineer." - Anonymous

> "No one knows what I do, and thanks to rust-stakeholder, neither do I." - Satisfied User

> "Since installing rust-stakeholder, my colleagues have stopped asking me for help because my work 'looks too advanced'." - Senior Imposter Engineer

## Tests? What tests?

Currently, this package has the same amount of test coverage as your excuses for missing deadlines - absolutely none.

Much like your actual development skills while using this tool, tests are purely theoretical at this point. But | <details><summary>blazi...</summary><p>blazingly-fast, meme, rust</p></details> | 1811 | https://github.com/giacomo-b/rust-stakeholder |
| **[Second-Me](https://github.com/mindverse/Second-Me)** | Train your AI self, amplify you, bridge the world | Python | # Second Me: Your AI Self

![Second Me](https://github.com/mindverse/Second-Me/blob/master/images/cover.png)

<div align="center">
  
[![Homepage](https://img.shields.io/badge/Second_Me-Homepage-blue?style=flat-square&logo=homebridge)](https://www.secondme.io/)
[![Report](https://img.shields.io/badge/Paper-arXiv-red?style=flat-square&logo=arxiv)](https://arxiv.org/abs/2503.08102)
[![Discord](https://img.shields.io/badge/Chat-Discord-5865F2?style=flat-square&logo=discord&logoColor=white)](https://discord.gg/GpWHQNUwrg)
[![Twitter](https://img.shields.io/badge/Follow-@SecondMe_AI-1DA1F2?style=flat-square&logo=x&logoColor=white)](https://x.com/SecondMe_AI1)
[![Reddit](https://img.shields.io/badge/Join-Reddit-FF4500?style=flat-square&logo=reddit&logoColor=white)](https://www.reddit.com/r/SecondMeAI/)

</div>

## Our Vision

At Second Me, we believe in creating an AI that enhances individuality rather than diminishing it. Our goal is to empower users to craft their own AI selvesâ€”an open-source prototype that preserves personal identity, delivers context, and protects interests. 

With **Second Me**, your AI is locally trained and hosted, ensuring you maintain control over your data while being globally connected. This platform serves as your digital identity interface, fostering collaboration among AI selves and enabling the development of innovative AI applications.

## Key Features

### **Train Your AI Self** with AI-Native Memory
Start training your Second Me today with your own memories! Our Hierarchical Memory Modeling (HMM) and Me-Alignment Algorithm allow your AI to capture your identity, understand context, and reflect you authentically.

<p align="center">
  <img src="https://github.com/user-attachments/assets/a84c6135-26dc-4413-82aa-f4a373c0ff89" width="94%" />
</p>

### **Scale Your Intelligence** on the Second Me Network
Launch your AI self from your laptop onto our decentralized network, allowing others and applications to connect with your permission, sharing your context as your digital identity.

<p align="center">
  <img src="https://github.com/user-attachments/assets/9a74a3f4-d8fd-41c1-8f24-534ed94c842a" width="94%" />
</p>

### Build Tomorrowâ€™s Apps with Second Me
- **Roleplay**: Switch personas for different scenarios.
- **AI Space**: Collaborate with other Second Mes to generate ideas or solve problems.

<p align="center">
  <img src="https://github.com/user-attachments/assets/bc6125c1-c84f-4ecc-b620-8932cc408094" width="94%" />
</p>

### 100% **Privacy and Control**
Your information and intelligence remain local and completely private, unlike traditional centralized AI systems.

## Getting Started

### Prerequisites
- macOS operating system
- Git installed
- Homebrew (recommended for system dependencies)
- Xcode Command Line Tools

#### Installing Xcode Command Line Tools
To install Xcode Command Line Tools, run:
```bash
xcode-select --install
```
Then, accept the license agreement:
```bash
sudo xcodebuild -license accept
```

### Installation and Setup

1. Clone the repository:
```bash
git clone git@github.com:Mindverse/Second-Me.git
cd Second-Me
```

2. Set up the environment:
Using make:
```bash
make setup
```
Or use the setup script directly:
```bash
./scripts/setup.sh
```

3. Start the service:
Using make:
```bash
make start
```
Or use the script directly:
```bash
./scripts/start.sh
```

4. Access the service:
Open your browser and visit `http://localhost:3000`.

5. For help and more commands:
Using make:
```bash
make help
```

## Tutorial
Follow the [User  tutorial](https://second-me.gitbook.io/a-new-ai-species-making-we-matter-again) to build your Second Me.

## Coming Soon ğŸš€

### ğŸ”¬ Model Enhancement Features
- **Long Chain-of-Thought Training Pipeline**
- **Direct Preference Optimization for L2 Model**
- **Data Filtering for Training**
- **Apple Silicon Support**

### ğŸ› ï¸ Product Features
- **Natural Language Memory Summarization**

## Contributing

We welcome contributions! Check out our [Contribution Guide](./CONTR |  | 1481 | https://github.com/mindverse/Second-Me |
| **[cursor-talk-to-figma-mcp](https://github.com/sonnylazuardi/cursor-talk-to-figma-mcp)** | Cursor Talk To Figma MCP | TypeScript | # Cursor Talk to Figma MCP

This project implements a Model Context Protocol (MCP) integration between Cursor AI and Figma, allowing Cursor to communicate with Figma for reading designs and modifying them programmatically.

## Project Structure

- **`src/talk_to_figma_mcp/`**: TypeScript MCP server for Figma integration.
- **`src/cursor_mcp_plugin/`**: Figma plugin for communicating with Cursor.
- **`src/socket.ts`**: WebSocket server that facilitates communication between the MCP server and Figma plugin.

## Get Started

1. **Install Bun** if you haven't already:

   ```bash
   curl -fsSL https://bun.sh/install | bash
   ```

2. **Run setup**; this will also install MCP in your Cursor's active project:

   ```bash
   bun setup
   ```

3. **Start the WebSocket server**:

   ```bash
   bun start
   ```

4. **Install the Figma Plugin**.

## Quick Video Tutorial

[![image](images/tutorial.jpg)](https://www.linkedin.com/posts/sonnylazuardi_just-wanted-to-share-my-latest-experiment-activity-7307821553654657024-yrh8)

## Manual Setup and Installation

### MCP Server: Integration with Cursor

Add the server to your Cursor MCP configuration in `~/.cursor/mcp.json`:

```json
{
  "mcpServers": {
    "TalkToFigma": {
      "command": "bun",
      "args": [
        "/path/to/cursor-talk-to-figma-mcp/src/talk_to_figma_mcp/server.ts"
      ]
    }
  }
}
```

### WebSocket Server

Start the WebSocket server:

```bash
bun run src/socket.ts
```

### Figma Plugin

1. In Figma, go to **Plugins > Development > New Plugin**.
2. Choose **"Link existing plugin"**.
3. Select the `src/cursor_mcp_plugin/manifest.json` file.
4. The plugin should now be available in your Figma development plugins.

## Usage

1. Start the WebSocket server.
2. Install the MCP server in Cursor.
3. Open Figma and run the Cursor MCP Plugin.
4. Connect the plugin to the WebSocket server by joining a channel using `join_channel`.
5. Use Cursor to communicate with Figma using the MCP tools.

## MCP Tools

The MCP server provides the following tools for interacting with Figma:

### Document & Selection

- **`get_document_info`**: Get information about the current Figma document.
- **`get_selection`**: Get information about the current selection.
- **`get_node_info`**: Get detailed information about a specific node.

### Creating Elements

- **`create_rectangle`**: Create a new rectangle with position, size, and optional name.
- **`create_frame`**: Create a new frame with position, size, and optional name.
- **`create_text`**: Create a new text node with customizable font properties.

### Modifying Text Content

- **`set_text_content`**: Set the text content of an existing text node.

### Styling

- **`set_fill_color`**: Set the fill color of a node (RGBA).
- **`set_stroke_color`**: Set the stroke color and weight of a node.
- **`set_corner_radius`**: Set the corner radius of a node with optional per-corner control.

### Layout & Organization

- **`move_node`**: Move a node to a new position.
- **`resize_node`**: Resize a node with new dimensions.
- **`delete_node`**: Delete a node.

### Components & Styles

- **`get_styles`**: Get information about local styles.
- **`get_local_components`**: Get information about local components.
- **`get_team_components`**: Get information about team components.
- **`create_component_instance`**: Create an instance of a component.

### Export & Advanced

- **`export_node_as_image`**: Export a node as an image (PNG, JPG, SVG, or PDF).
- **`execute_figma_code`**: Execute arbitrary JavaScript code in Figma (use with caution).

### Connection Management

- **`join_channel`**: Join a specific channel to communicate with Figma.

## Development

### Building the Figma Plugin

1. Navigate to the Figma plugin directory:

   ```bash
   cd src/cursor_mcp_plugin
   ```

2. Edit `code.js` and `ui.html`.

## Best Practices

When working with the Figma MCP:

1. Always join a channel before sending commands.
2. Get document overview using `get_document_info` first.
3. Check current selection with `get_selection` before modifications.
4 | <details><summary>agent...</summary><p>agent, agentic, agentic-ai, ai, cursor, design, figma, mcp</p></details> | 1257 | https://github.com/sonnylazuardi/cursor-talk-to-figma-mcp |
| **[anubis](https://github.com/TecharoHQ/anubis)** | Weighs the soul of incoming HTTP requests using proof-of-work to stop AI crawlers | Go | # Anubis

<center>
<img width=256 src="./cmd/anubis/static/img/happy.webp" alt="A smiling chibi dark-skinned anthro jackal with brown hair and tall ears looking victorious with a thumbs-up" />
</center>

![enbyware](https://pride-badges.pony.workers.dev/static/v1?label=enbyware&labelColor=%23555&stripeWidth=8&stripeColors=FCF434%2CFFFFFF%2C9C59D1%2C2C2C2C)
![GitHub Issues or Pull Requests by label](https://img.shields.io/github/issues/TecharoHQ/anubis)
![GitHub go.mod Go version](https://img.shields.io/github/go-mod/go-version/TecharoHQ/anubis)
![language count](https://img.shields.io/github/languages/count/TecharoHQ/anubis)
![repo size](https://img.shields.io/github/repo-size/TecharoHQ/anubis)

Anubis [weighs the soul of your connection](https://en.wikipedia.org/wiki/Weighing_of_souls) using a SHA-256 proof-of-work challenge to protect upstream resources from scraper bots.

## Overview

Installing and using Anubis will likely prevent your website from being indexed by some search engines. This is considered a feature, not a bug. The aggressive behavior of AI scraper bots has necessitated this approach. While Cloudflare is often sufficient for protection, Anubis is an alternative for those who cannot or do not wish to use it.

If you want to try this out, connect to [anubis.techaro.lol](https://anubis.techaro.lol).

## Support

If you encounter any issues while running Anubis, please [open an issue](https://github.com/TecharoHQ/anubis/issues/new?template=Blank+issue) and tag it with the Anubis tag. Be sure to include all relevant information for diagnosis.

For live chat support, consider joining the [Patreon](https://patreon.com/cadey) and asking in the Patron Discord in the `#anubis` channel.

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=TecharoHQ/anubis&type=Date)](https://www.star-history.com/#TecharoHQ/anubis&Date) | <details><summary>defen...</summary><p>defense, security</p></details> | 885 | https://github.com/TecharoHQ/anubis |
| **[unity-mcp](https://github.com/justinpbarnett/unity-mcp)** | A Unity MCP server that allows MCP clients like Claude Desktop or Cursor to perform Unity Editor actions. | C# | # Unity MCP Package

The Unity MCP Package is an innovative tool designed to facilitate seamless communication between Unity and Large Language Models (LLMs) such as Claude Desktop through the **Model Context Protocol (MCP)**. This package acts as a bridge, enabling developers to send commands to and receive responses from MCP-compliant tools, thereby automating workflows, manipulating assets, and controlling the Unity Editor programmatically.

Welcome to the initial release of this open-source project! Whether you are looking to integrate LLMs into your Unity workflow or contribute to an exciting new tool, I appreciate you taking the time to check out this project.

## Overview

The Unity MCP Server provides a bidirectional communication channel between Unity (via C#) and a Python server, enabling:

- **Asset Management**: Create, import, and manipulate Unity assets programmatically.
- **Scene Control**: Manage scenes, objects, and their properties.
- **Material Editing**: Modify materials and their properties.
- **Script Integration**: View, create, and update Unity scripts.
- **Editor Automation**: Control Unity Editor functions like undo, redo, play, and build.

This project is perfect for developers who want to leverage LLMs to enhance their Unity projects or automate repetitive tasks.

## Installation

### Prerequisites

- Unity 2020.3 LTS or newer (âš ï¸ only works in URP projects currently)
- Python 3.7 or newer
- uv package manager

**If you're on Mac, please install uv as follows:**

```bash
brew install uv
```

**On Windows:**

```powershell
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```

Then, add to your PATH:

```bash
set Path=%USERPROFILE%\.local\bin;%Path%
```

**On Linux:**

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

For further installation instructions, visit the [Install uv](https://docs.astral.sh/uv/getting-started/installation/) website.

**âš ï¸ Do not proceed before installing UV**

### Unity Package Installation

1. **Add the Unity Package**

   - Open Unity Package Manager (`Window > Package Manager`)
   - Click the `+` button and select `Add package from git URL`
   - Enter: `https://github.com/justinpbarnett/unity-mcp.git`

2. **Set Up Python Environment**
   - Navigate to the Python directory in your project:
     - If installed as a package: `Library/PackageCache/com.justinpbarnett.unity-mcp/Python`
     - If installed locally: `Assets/unity-mcp/Python`
   - Install dependencies:
     ```bash
     uv venv
     uv pip install -e .
     ```

### MCP Client Integration

1. Open the Unity MCP window (`Window > Unity MCP`)
2. Click the "Auto Configure" button for your desired MCP client.
3. The status indicator should show green with a "Configured" message.

Alternatively, manually configure your MCP client:

1. Open the Unity MCP window (`Window > Unity MCP`)
2. Click the "Manually Configure" button for your desired MCP client.
3. Copy the JSON code below to the config file.

```json
{
  "mcpServers": {
    "unityMCP": {
      "command": "uv",
      "args": [
        "--directory",
        "/path/to/your/unity-mcp/Python",
        "run",
        "server.py"
      ]
    }
  }
}
```

Replace `/path/to/your/unity-mcp/Python` with the actual path to the Unity MCP Python directory.

**âš ï¸ Only run one instance of the MCP server (either on Cursor or Claude Desktop), not both.**

4. **Start Claude Desktop or Cursor**
   - Launch your preferred tool.
   - The Unity MCP Server will automatically start and connect.

## Usage

Once configured, you can use the MCP Client to interact with Unity directly through their chat interface.

## Features

- **Bidirectional Communication**: Seamlessly send and receive data between Unity and LLMs.
- **Asset Management**: Import assets, instantiate prefabs, and create new prefabs programmatically.
- **Scene Control**: Open, save, and modify scenes, plus create and manipulate game objects.
- **Material Editing**: Apply and modify materials with ease.
- **Script Integration**: Create, view, and update C# scripts within Unity.
- **Editor Automation**: Automate Unity Editor tasks like building projects or entering play mode.

## Contributing

We welcome contributions to make the Unity MCP Server even better! Here's how to contribute:

1. **Fork the Repository**  
   Fork [github.com/justinpbarnett/unity-mcp](https://github.com/justinpbarnett/unity-mcp) | <details><summary>ai, a...</summary><p>ai, ai-integration, mcp, unity</p></details> | 717 | https://github.com/justinpbarnett/unity-mcp |
| **[Skywork-R1V](https://github.com/SkyworkAI/Skywork-R1V)** | Pioneering Multimodal Reasoning with CoT | Python | # Skywork-R1V: Pioneering Multimodal Reasoning with CoT
<font size=7><div align='center' > [[ğŸ“–Technical Report](https://github.com/SkyworkAI/Skywork-R1V/blob/main/Skywork_R1V.pdf)] [[ğŸ¤— Skywork-R1V-38B](https://huggingface.co/Skywork/Skywork-R1V-38B)] </div></font>

Welcome to the Skywork-R1V repository! Here, you'll find the model weights and inference code for our state-of-the-art open-sourced multimodal reasoning model, enabling advanced visual and logical thinking.

## ğŸ”¥News
**Mar 18, 2025**: We are thrilled to introduce Skywork R1V, the first industry open-sourced multimodal reasoning model with advanced visual chain-of-thought capabilities, pushing the boundaries of AI-driven vision and logical inference! ğŸš€

<div align="center">
  <table>
    <tr>
      <td>
        <img src="https://github.com/SkyworkAI/Skywork-R1V/blob/main/imgs/math_r1v.gif" width="450" height="400" alt="math_r1v" />
      </td>
      <td>
        <img src="https://github.com/SkyworkAI/Skywork-R1V/blob/main/imgs/Chemistry_cn.gif" width="450" height="400" alt="chemistry_1" />
      </td>
    </tr>
  </table>
</div>

## Features
- **Visual Chain-of-Thought**: Enables multi-step logical reasoning on visual inputs, breaking down complex image-based problems into manageable steps.
- **Mathematical & Scientific Analysis**: Capable of solving visual math problems and interpreting scientific/medical imagery with high precision.
- **Cross-Modal Understanding**: Seamlessly integrates text and images for richer, context-aware comprehension.

## Evaluation 
<div align="center">
  <b>Evaluation results of state-of-the-art LLMs and VLMs</b>
</div>
<table>
  <thead>
    <tr>
      <th></th>
      <th align="center"><strong>Size</strong></th>
      <th align="center"><strong>Vision</strong></th>
      <th align="center" colspan="3"><strong>Reasoning</strong></th>
      <th align="center" colspan="3"><strong>Vision</strong></th>
    </tr>
    <tr>
      <th></th>
      <th align="center"></th>
      <th></th>
      <th align="center"><strong>MATH-500</strong></th>
      <th align="center"><strong>AIME 2024</strong></th>
      <th align="center"><strong>GPQA</strong></th>
      <th align="center"><strong>MathVista(mini)</strong></th>
      <th align="center"><strong>MMMU(Val)</strong></th>
    </tr>
    <tr>
      <th></th>
      <th align="center"></th>
      <th></th>
      <th align="center">pass@1</th>
      <th align="center">pass@1</th>
      <th align="center">pass@1</th>
      <th align="center">pass@1</th>
      <th align="center">pass@1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Qwen2.5-72B-Instruct</td>
      <td align="center">72B</td>
      <td align="center">âŒ</td>
      <td align="center">80.0</td>
      <td align="center">23.3</td>
      <td align="center">49.0</td>
      <td align="center">-</td>
      <td align="center">-</td>
    </tr>
    <tr>
      <td>Deepseek V3</td>
      <td align="center">671B</td>
      <td align="center">âŒ</td>
      <td align="center">90.2</td>
      <td align="center">39.2</td>
      <td align="center">59.1</td>
      <td align="center">-</td>
      <td align="center">-</td>
    </tr>
    <tr>
      <td>Deepseek R1</td>
      <td align="center">671B</td>
      <td align="center">âŒ</td>
      <td align="center">97.3</td>
      <td align="center">79.8</td>
 | <details><summary>deeps...</summary><p>deepseek-r1, llm, mllm</p></details> | 663 | https://github.com/SkyworkAI/Skywork-R1V |
| **[open-wegram-bot](https://github.com/wozulong/open-wegram-bot)** | ã€é›¶è´¹ç”¨ã€‘ä¸€ä¸ªè®©äººå‘¼å¸é¡ºç•…çš„ Telegram åŒå‘ç§èŠæœºå™¨äºº ğŸ¤– / [Zero Cost] A Smooth-Operating Two-Way Private Messaging Telegram Bot ğŸ¤–  | JavaScript | # Open Wegram Bot - OWB
## ä¸€ä¸ªè®©äººå‘¼å¸é¡ºç•…çš„ Telegram åŒå‘ç§èŠæœºå™¨äºº ğŸ¤–ï¼ˆé›¶è´¹ç”¨ï¼‰
### *LivegramBot ä¸æ­»ï¼Œæˆ˜æ–—ä¸æ­¢ï¼*

ç®€ä½“ä¸­æ–‡ | [English](README_EN.md) 

è¿™æ˜¯ä¸€ä¸ªåŸºäº Cloudflare Worker / Vercel çš„ Telegram åŒå‘ç§èŠæœºå™¨äººï¼Œæ— éœ€æœåŠ¡å™¨ã€æ— éœ€æ•°æ®åº“ã€æ— éœ€è‡ªå·±çš„åŸŸåå³å¯è½»æ¾éƒ¨ç½²ã€‚

ç”¨æˆ·å¯ä»¥é€šè¿‡æ‚¨çš„æœºå™¨äººå‘æ‚¨å‘é€æ¶ˆæ¯ï¼Œæ‚¨å¯ä»¥ç›´æ¥å›å¤è¿™äº›æ¶ˆæ¯ï¼Œå®ç°åŒå‘é€šä¿¡ã€‚

## âœ¨ ç‰¹è‰²åŠŸèƒ½

- ğŸ”„ **åŒå‘é€šä¿¡** - è½»æ¾æ¥æ”¶å’Œå›å¤æ¥è‡ªç”¨æˆ·çš„æ¶ˆæ¯
- ğŸ’¾ **æ— éœ€æ•°æ®åº“** - å®Œå…¨æ— çŠ¶æ€è®¾è®¡ï¼Œé›¶å­˜å‚¨æˆæœ¬
- ğŸŒ **æ— éœ€è‡ªå·±çš„åŸŸå** - ä½¿ç”¨ Cloudflare Worker æä¾›çš„å…è´¹åŸŸå
- ğŸš€ **è½»é‡çº§éƒ¨ç½²** - å‡ åˆ†é’Ÿå†…å³å¯å®Œæˆè®¾ç½®
- ğŸ’° **é›¶æˆæœ¬è¿è¡Œ** - åœ¨ Cloudflare å…è´¹è®¡åˆ’èŒƒå›´å†…ä½¿ç”¨
- ğŸ”’ **å®‰å…¨å¯é ** - ä½¿ç”¨ Telegram å®˜æ–¹ API å’Œå®‰å…¨ä»¤ç‰Œ
- ğŸ”Œ **å¤šæœºå™¨äººæ”¯æŒ** - ä¸€ä¸ªéƒ¨ç½²å¯æ³¨å†Œå¤šä¸ªç§èŠæœºå™¨äºº
- ğŸ› ï¸ **å¤šç§éƒ¨ç½²æ–¹å¼** - æ”¯æŒ GitHub ä¸€é”®éƒ¨ç½²ã€Vercel ä¸€é”®éƒ¨ç½²ã€Wrangler CLI å’Œ Dashboard éƒ¨ç½²

## ğŸ› ï¸ å‰ç½®è¦æ±‚

- Cloudflare è´¦å·
- Telegram è´¦å·
- ä¸€ä¸ªç§‘å­¦å·¥å…·ï¼ˆä»…è®¾ç½®é˜¶æ®µéœ€è¦ï¼Œç”¨äºè®¿é—® Worker é»˜è®¤åŸŸåï¼Œè‡ªç»‘åŸŸåæ— è§†ï¼‰

## ğŸ“ è®¾ç½®æ­¥éª¤

### 1. è·å– Telegram UID

> [!NOTE]
> æ‚¨éœ€è¦çŸ¥é“è‡ªå·±çš„ Telegram ç”¨æˆ· ID (UID)ï¼Œè¿™æ˜¯ä¸€ä¸²æ•°å­—ï¼Œç”¨äºå°†æ¶ˆæ¯è½¬å‘ç»™æ‚¨ã€‚

æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è·å–ï¼š

- å‘ [@userinfobot](https://t.me/userinfobot) å‘é€ä»»æ„æ¶ˆæ¯ï¼Œå®ƒä¼šå‘Šè¯‰æ‚¨è‡ªå·±çš„ UID

è¯·è®°ä¸‹æ‚¨çš„æ•°å­— IDï¼ˆä¾‹å¦‚ï¼š`123456789`ï¼‰ã€‚

### 2. åˆ›å»º Telegram Bot

1. åœ¨ Telegram ä¸­æœç´¢å¹¶æ‰“å¼€ [@BotFather](https://t.me/BotFather)
2. å‘é€ `/newbot` å‘½ä»¤
3. æŒ‰ç…§æç¤ºè®¾ç½®æ‚¨çš„æœºå™¨äººåç§°å’Œç”¨æˆ·åï¼ˆç”¨æˆ·åå¿…é¡»ä»¥ `bot` ç»“å°¾ï¼‰
4. æˆåŠŸåï¼ŒBotFather ä¼šå‘ç»™æ‚¨ä¸€ä¸ª Bot API Tokenï¼ˆæ ¼å¼ç±»ä¼¼ï¼š`000000000:ABCDEFGhijklmnopqrstuvwxyz`ï¼‰
5. è¯·å®‰å…¨ä¿å­˜è¿™ä¸ª Bot API Token

### 3. é€‰æ‹©éƒ¨ç½²æ–¹å¼

#### æ–¹æ³•ä¸€ï¼šGitHub ä¸€é”®éƒ¨ç½²ï¼ˆæ¨è â­ï¼‰

è¿™æ˜¯æœ€ç®€å•çš„éƒ¨ç½²æ–¹å¼ï¼Œæ— éœ€æœ¬åœ°å¼€å‘ç¯å¢ƒï¼Œç›´æ¥é€šè¿‡ GitHub ä»“åº“éƒ¨ç½²ã€‚

1. Fork æˆ–å…‹éš†æœ¬ä»“åº“åˆ°æ‚¨çš„ GitHub è´¦æˆ·
2. ç™»å½• [Cloudflare Dashboard](https://dash.cloudflare.com/)
3. å¯¼èˆªåˆ° **Workers & Pages** éƒ¨åˆ†
4. ç‚¹å‡» **Create Application**
5. é€‰æ‹© **Connect to Git**
6. æˆæƒ Cloudflare è®¿é—®æ‚¨çš„ GitHubï¼Œå¹¶é€‰æ‹©æ‚¨ fork çš„ä»“åº“
7. é…ç½®éƒ¨ç½²è®¾ç½®ï¼š
   - **Project name**ï¼šè®¾ç½®æ‚¨çš„é¡¹ç›®åç§°ï¼ˆä¾‹å¦‚ `open-wegram-bot`ï¼‰
   - **Production branch**ï¼šé€‰æ‹©ä¸»åˆ†æ”¯ï¼ˆé€šå¸¸æ˜¯ `master`ï¼‰
   - å…¶ä»–è®¾ç½®ä¿æŒé»˜è®¤
8. é…ç½®ç¯å¢ƒå˜é‡ï¼š
   - ç‚¹å‡» **Environment Variables**
   - æ·»åŠ  `PREFIX`ï¼ˆä¾‹å¦‚ï¼š`public`ï¼‰
   - æ·»åŠ  `SECRET_TOKEN`ï¼ˆå¿…é¡»åŒ…å«å¤§å°å†™å­—æ¯å’Œæ•°å­—ï¼Œé•¿åº¦è‡³å°‘16ä½ï¼‰ï¼Œå¹¶æ ‡è®°ä¸º**åŠ å¯†**
9. ç‚¹å‡» **Save and Deploy** æŒ‰é’®å®Œæˆéƒ¨ç½²

è¿™ç§æ–¹å¼çš„ä¼˜ç‚¹æ˜¯ï¼šå½“æ‚¨æ›´æ–° GitHub ä»“åº“æ—¶ï¼ŒCloudflare ä¼šè‡ªåŠ¨é‡æ–°éƒ¨ç½²æ‚¨çš„ Workerã€‚

#### æ–¹æ³•äºŒï¼šVercel ä¸€é”®éƒ¨ç½²

Vercel æä¾›äº†å¦ä¸€ç§ç®€å•çš„éƒ¨ç½²æ–¹å¼ï¼Œä¹Ÿæ”¯æŒä» GitHub ä»“åº“è‡ªåŠ¨éƒ¨ç½²ã€‚

1. ç‚¹å‡»ä¸‹æ–¹çš„"Deploy with Vercel"æŒ‰é’®ï¼š

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fwozulong%2Fopen-wegram-bot&env=SECRET_TOKEN,PREFIX&envDescription=é…ç½®æ‚¨çš„æœºå™¨äººå‚æ•°&project-name=open-wegram-bot&repository-name=open-wegram-bot)

2. æŒ‰ç…§ Vercel çš„æç¤ºå®Œæˆéƒ¨ç½²æµç¨‹
3. é…ç½®ç¯å¢ƒå˜é‡ï¼š
   - `PREFIX`ï¼šè®¾ç½®ä¸ºæ‚¨æƒ³è¦çš„ URL å‰ç¼€ï¼ˆä¾‹å¦‚ `public`ï¼‰
   - `SECRET_TOKEN`ï¼šè®¾ç½®ä¸€ä¸ªå®‰å…¨çš„ä»¤ç‰Œï¼ˆå¿…é¡»åŒ…å«å¤§å°å†™å­—æ¯å’Œæ•°å­—ï¼Œé•¿åº¦è‡³å°‘16ä½ï¼‰
4. å®Œæˆéƒ¨ç½²åï¼ŒVercel ä¼šæä¾›ä¸€ä¸ªåŸŸåï¼Œå¦‚ ` | <details><summary>teleg...</summary><p>telegram, telegram-bot, telegram-bots</p></details> | 646 | https://github.com/wozulong/open-wegram-bot |
| **[Classless.css](https://github.com/DigitallyTailored/Classless.css)** | A lightweight, classless CSS framework that makes simple websites look better without requiring any additional markup. | CSS | # Classless.css

A lightweight CSS framework for websites with impeccable taste but zero desire to add classes.

## Preview

Check out the live demo: [Classless.css Demo](https://digitallytailored.github.io/Classless.css/)

## How to Use

To use Classless.css in your project, simply add the following line to the `<head>` section of your HTML:

```html
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/digitallytailored/classless@latest/classless.min.css">
```

This link always points to the latest version of the minified CSS.

## Features

- **No classes required**: Enjoy a clean and simple markup without the need for CSS classes.
- **Responsive design**: Automatically adapts to different screen sizes for a seamless experience.
- **Dark mode support**: Easily switch between light and dark themes.
- **Typography enhancements**: Improved font styles for better readability.
- **Form styling**: Styled forms that look great out of the box.
- **Table improvements**: Enhanced table styles for a more polished appearance.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is open source and available under the [MIT License](https://opensource.org/license/mit). |  | 589 | https://github.com/DigitallyTailored/Classless.css |
| **[DAPO](https://github.com/BytedTsinghua-SIA/DAPO)** | An Open-source RL System from ByteDance Seed and Tsinghua AIR |  | # DAPO: an Open-source RL System from ByteDance Seed and Tsinghua AIR

<div align='center'>
[![Paper](https://img.shields.io/badge/paper-5f16a8?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2503.14476)
[![Blog](https://img.shields.io/badge/Blog-3858bf?style=for-the-badge&logo=homepage&logoColor=white)](https://DAPO-SIA.github.io/)
[![Dataset](https://img.shields.io/badge/Datasets-4d8cd8?style=for-the-badge&logo=huggingface&logoColor=white)](https://huggingface.co/datasets/BytedTsinghua-SIA/DAPO-Math-17k)
[![Weights](https://img.shields.io/badge/Model%20Weights(coming%20soon)-63cad3?style=for-the-badge&logo=huggingface&logoColor=white)](https://github.com/BytedTsinghua-SIA/DAPO)
</div>

We are excited to announce the release of **DAPO**, a fully open-sourced system for large-scale reinforcement learning (RL) with large language models (LLMs). DAPO stands for **D**ecoupled Clip and **D**ynamic s**A**mpling **P**olicy **O**ptimization, which is a novel algorithm designed to achieve state-of-the-art performance in LLM RL tasks.

Through this initiative, we aim to provide the research community and society with practical access to scalable reinforcement learning, fostering innovation and collaboration in this exciting field. Our system builds upon the impressive [verl](https://github.com/volcengine/verl) framework, and we extend our gratitude to the developers for their outstanding work.

## Discussions Welcomed

If you have any questions or comments regarding our paper or the DAPO system, please feel free to open issues on our GitHub repository. We welcome discussions and feedback!

## Key Results

### AIME 2024 Performance

ğŸš€ **DAPO** achieves an impressive score of **50 points** on the AIME 2024 benchmark using the Qwen2.5-32B base model, surpassing the previous state-of-the-art (SoTA) model, DeepSeek-R1-Zero-Qwen-32B, while utilizing 50% fewer training steps.

![Performance Score](img/score.png)

### Metric Supervision during Training

1. **Length Stability and Growth**: The gradual increase in response length promotes exploration, enabling the model to learn complex reasoning behaviors, thus enhancing training stability and performance.
  
2. **Reward Score Stability**: A consistent rise in reward signals indicates that the model is effectively fitting the training distribution, ensuring a robust and stable learning process.

3. **Entropy and Mean Probability Trend**: A controlled increase in entropy, following an initial decrease, maintains a balance between exploration and exploitation, preventing overfitting and promoting sustained model performance.

![Training Metrics](img/dynamic.png)

## Reproducibility

To support the research community, we are providing comprehensive resources for reproducing our RL training process, including algorithm details, datasets, and infrastructure.

### Datasets

We offer both training and validation datasets for DAPO:

- **Training**: [DAPO-Math-17k](https://huggingface.co/datasets/BytedTsinghua-SIA/DAPO-Math-17k) - A carefully curated math dataset.
- **Validation**: [AIME 2024](https://huggingface.co/datasets/BytedTsinghua-SIA/AIME-2024).

### Training

We provide an [out-of-the-box](https://github.com/volcengine/verl/blob/gm-tyx/puffin/main/recipe/dapo) script for reproducing DAPO training. The core code and quickstart instructions can be found in the [README](https://github.com/volcengine/verl/blob/gm-tyx/puffin/main/recipe/dapo/README.md). The scripts include:

- [Datasets Preparation](https://github.com/volcengine/verl/blob/gm-tyx/puffin/main/recipe/dapo/prepare_dapo_data.sh)
- [DAPO w/o Dynamic Sampling -- AIME 44](https://github.com/volcengine/verl/blob/gm-tyx/puffin/main/recipe/dapo/run_dapo_early_qwen2.5_32b.sh)
- [DAPO Full -- AIME 50](https://github.com/volcengine/verl/blob/gm-tyx/puffin/main/recipe/dapo/run_dapo_qwen2.5_32b.sh)

**Note**:
 |  | 586 | https://github.com/BytedTsinghua-SIA/DAPO |
| **[rag-zero-to-hero-guide](https://github.com/KalyanKS-NLP/rag-zero-to-hero-guide)** | Comprehensive guide to learn RAG from basics to advanced.  | Jupyter Notebook | Request error occurred:  | <details><summary>ai-en...</summary><p>ai-engineer, generative-ai, large-language-models, llm-engineer, llm-rag, llms, retrieval-augmented-generation</p></details> | 425 | https://github.com/KalyanKS-NLP/rag-zero-to-hero-guide |
| **[ableton-mcp](https://github.com/ahujasid/ableton-mcp)** |  | Python | # AbletonMCP - Ableton Live Model Context Protocol Integration

AbletonMCP connects Ableton Live to Claude AI through the Model Context Protocol (MCP), allowing Claude to directly interact with and control Ableton Live. This integration enables prompt-assisted music production, track creation, and Live session manipulation.

### Join the Community

Give feedback, get inspired, and build on top of the MCP: [Discord](https://discord.gg/SheqY3UW). Made by [Siddharth](https://x.com/sidahuj).

## Features

- **Two-way communication**: Connect Claude AI to Ableton Live through a socket-based server.
- **Track manipulation**: Create, modify, and manipulate MIDI and audio tracks.
- **Instrument and effect selection**: Claude can access and load the right instruments, effects, and sounds from Ableton's library.
- **Clip creation**: Create and edit MIDI clips with notes.
- **Session control**: Start and stop playback, fire clips, and control transport.

## Components

The system consists of two main components:

1. **Ableton Remote Script** (`Ableton_Remote_Script/__init__.py`): A MIDI Remote Script for Ableton Live that creates a socket server to receive and execute commands.
2. **MCP Server** (`server.py`): A Python server that implements the Model Context Protocol and connects to the Ableton Remote Script.

## Installation

### Prerequisites

- Ableton Live 10 or newer
- Python 3.8 or newer
- [uv package manager](https://astral.sh/uv)

If you're on Mac, please install uv as:
```bash
brew install uv
```

Otherwise, install from [uv's official website](https://docs.astral.sh/uv/getting-started/installation/).

âš ï¸ Do not proceed before installing UV.

### Claude for Desktop Integration

[Follow along with the setup instructions video](https://youtu.be/iJWJqyVuPS8).

1. Go to Claude > Settings > Developer > Edit Config > `claude_desktop_config.json` to include the following:

```json
{
    "mcpServers": {
        "AbletonMCP": {
            "command": "uvx",
            "args": [
                "ableton-mcp"
            ]
        }
    }
}
```

### Cursor Integration

Run AbletonMCP without installing it permanently through uvx. Go to Cursor Settings > MCP and paste this as a command:

```bash
uvx ableton-mcp
```

âš ï¸ Only run one instance of the MCP server (either on Cursor or Claude Desktop), not both.

### Installing the Ableton Remote Script

[Follow along with the setup instructions video](https://youtu.be/iJWJqyVuPS8).

1. Download the `AbletonMCP_Remote_Script/__init__.py` file from this repo.

2. Copy the folder to Ableton's MIDI Remote Scripts directory. Different OS and versions have different locations. **One of these should work, you might have to look**:

   **For macOS:**
   - Method 1: Go to Applications > Right-click on Ableton Live app â†’ Show Package Contents â†’ Navigate to:
     `Contents/App-Resources/MIDI Remote Scripts/`
   - Method 2: If it's not there in the first method, use the direct path (replace XX with your version number):
     `/Users/[Username]/Library/Preferences/Ableton/Live XX/User Remote Scripts`
   
   **For Windows:**
   - Method 1:
     `C:\Users\[Username]\AppData\Roaming\Ableton\Live x.x.x\Preferences\User Remote Scripts`
   - Method 2:
     `C:\ProgramData\Ableton\Live XX\Resources\MIDI Remote Scripts\`
   - Method 3:
     `C:\Program Files\Ableton\Live XX\Resources\MIDI Remote Scripts\`
   *Note: Replace XX with your Ableton version number (e.g., 10, 11, 12).*

3. Create a folder called 'AbletonMCP' in the Remote Scripts directory and paste the downloaded `__init__.py` file.

4. Launch Ableton Live.

5. Go to Settings/Preferences â†’ Link, Tempo & MIDI.

6. In the Control Surface dropdown, select "AbletonMCP".

7. Set Input and Output to "None".

## Usage

### Starting the Connection

1. Ensure the Ableton Remote Script is loaded in Ableton Live.
2. Make sure the MCP server is configured in Claude Desktop or Cursor.
3. The connection should be established automatically when you interact with Claude.

### Using with Claude

Once the config file has been set on Claude, and the remote script is running in Ableton, you will see a hammer icon with tools for the Ableton MCP.

## Capabilities

- Get session and track information |  | 395 | https://github.com/ahujasid/ableton-mcp |
| **[gemini-image-editing-nextjs-quickstart](https://github.com/google-gemini/gemini-image-editing-nextjs-quickstart)** | Get started with native image generation and editing using Gemini 2.0 and Next.js | TypeScript | This document provides a comprehensive overview of a Next.js quickstart project for generating and editing images using Google Gemini 2.0 Flash. Hereâ€™s a structured summary of the essential information included in the guide:

### Overview

- **Project Purpose**: The application allows users to generate images from text prompts and edit existing images based on natural language instructions. It maintains conversation context for iterative refinements.
- **Demo**: A hosted demo is available at [Hugging Face Spaces](https://huggingface.co/spaces/philschmid/image-generation-editing).

### Getting Started

1. **API Key**: 
   - Obtain your `GEMINI_API_KEY` from the [Google AI Studio](https://ai.google.dev/gemini-api/docs/api-key).
  
2. **Local Development Setup**:
   - Copy the example environment configuration:
     ```bash
     cp .env.example .env
     ```
   - Add your API key to the `.env` file:
     ```plaintext
     GEMINI_API_KEY=your_google_api_key
     ```
   - Install dependencies and start the development server:
     ```bash
     npm install
     npm run dev
     ```
   - Access the application at [http://localhost:3000](http://localhost:3000).

### Features

- **Image Generation**: Create images from text prompts.
- **Image Editing**: Modify uploaded images with natural language instructions.
- **Conversation History**: Keep track of context for refining image requests.
- **Responsive UI**: Built using Next.js and shadcn/ui for a smooth user experience.
- **Seamless Workflow**: Easy transition between creation and editing modes.
- **SDK Usage**: Utilizes the Google Generative AI JavaScript SDK.

### Basic Request Example

A sample JavaScript function to generate an image using the Google Generative AI SDK is provided:

```javascript
const { GoogleGenerativeAI } = require("@google/generative-ai");
const fs = require("fs");

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);

async function generateImage() {
  const contents = "Hi, can you create a 3d rendered image of a pig with wings and a top hat flying over a happy futuristic scifi city with lots of greenery?";

  const model = genAI.getGenerativeModel({
    model: "gemini-2.0-flash-exp",
    generationConfig: {
      responseModalities: ["Text", "Image"]
    }
  });

  try {
    const response = await model.generateContent(contents);
    for (const part of response.response.candidates[0].content.parts) {
      if (part.text) {
        console.log(part.text);
      } else if (part.inlineData) {
        const imageData = part.inlineData.data;
        const buffer = Buffer.from(imageData, "base64");
        fs.writeFileSync("gemini-native-image.png", buffer);
        console.log("Image saved as gemini-native-image.png");
      }
    }
  } catch (error) {
    console.error("Error generating content:", error);
  }
}
```

### Deployment Options

- **Vercel**: Easily deploy the application using Vercel.
- **Docker**: Build and run the application in a Docker container:
  1. Build the Docker image:
     ```bash
     docker build -t nextjs-gemini-image-editing .
     ```
  2. Run the container with your API key:
     ```bash
     docker run -p 3000:3000 -e GEMINI_API_KEY=your_google_api_key nextjs-gemini-image-editing
     ```
     Or use an environment file:
     ```bash
     docker run -p 3000:3000 --env-file .env nextjs-gemini-image-editing
     ```

### Technologies Used

- **Next.js**: Framework for building the web application.
- **Google Gemini 2.0 Flash**: AI model for image generation and editing.
- **shadcn/ui**: Component library for building user interfaces.

### License

This project is licensed under the Apache License 2.0. For details, see the [LICENSE](./LICENSE) file.

This summary encapsulates the key points of the project, making it easier for developers to understand and utilize the application effectively. | <details><summary>gemin...</summary><p>gemini, gemini-api</p></details> | 371 | https://github.com/google-gemini/gemini-image-editing-nextjs-quickstart |
| **[familytree](https://github.com/qiaoshouqing/familytree)** | A minimalist open-source family tree website project. | TypeScript | # Family Tree

English | [ä¸­æ–‡](./README.zh.md)

A family tree visualization project built with [Next.js](https://nextjs.org) for displaying and managing your family history and member relationships.

## Demo Website

You can visit [https://familytree.pomodiary.com/](https://familytree.pomodiary.com/) to see an online demonstration of this project.

## Features

- Visual representation of multiple generations of family members
- Relationship connections between family members
- Detailed personal information records
- Optional login authentication mechanism
- Fully customizable interface and data

## Quick Start

### Install Dependencies

```bash
npm install
# or
yarn install
# or
pnpm install
# or
bun install
```

### Configure the Project

1. Copy the environment variable template and configure it:

```bash
cp .env.local.example .env.local
```

2. Set your configurations in the `.env.local` file:

```
# Whether login authentication is required (true/false)
NEXT_PUBLIC_REQUIRE_AUTH=false

# Authentication mode (all: allow all family members, specific: only allow specific names)
AUTH_MODE=specific
# Specific user login name
SPECIFIC_NAME=ç™½æ™¯ç¦

# Surname configuration (for website title, description, and footer)
NEXT_PUBLIC_FAMILY_NAME=ç™½

# Application port configuration
PORT=3000
```

### Add Family Data

1. Create your family data file `family-data.json` in the `config` directory, you can refer to `family-data.example.json` or `family-data.json`.

2. Add your family member information in the following format:

```json
{
  "generations": [
    {
      "title": "First Generation",
      "people": [
        {
          "id": "person-id",
          "name": "Name",
          "info": "Person description",
          "fatherId": "Father's ID",
          "birthYear": 1900,
          "deathYear": 1980
        }
      ]
    }
  ]
}
```

Field descriptions:
- `id`: Unique identifier for each person, used to establish relationships
- `name`: Name
- `info`: Personal description, life summary, etc.
- `fatherId`: Father's ID, used to establish generational relationships
- `birthYear`: Birth year (optional)
- `deathYear`: Death year (optional)

### Run the Project

```bash
npm run dev
# or
yarn dev
# or
pnpm dev
# or
bun dev
```

Visit [http://localhost:3000](http://localhost:3000) to view your family tree.

## Data Format Details

Family data is stored in JSON format, organized by generations:

- Each generation has a title and a group of people
- Each person includes ID, name, information, and father's ID
- Parent-child relationships are established through `fatherId`
- You can add spouse, children, and other important information in the info field

Example:
```json
{
  "generations": [
    {
      "title": "First Generation",
      "people": [
        {
          "id": "ancestor",
          "name": "Ancestor",
          "info": "Family founder, born in 1850",
          "birthYear": 1850
        }
      ]
    },
    {
      "title": "Second Generation",
      "people": [
        {
          "id": "second-gen-1",
          "name": "First Son",
          "info": "Born in 1880, wife Wang",
          "fatherId": "ancestor",
          "birthYear": 1880,
          "deathYear": 1950
        },
        {
          "id": "second-gen-2",
          "name": "Second Son",
          "info": "Born in 1885, wife Li",
          "fatherId": "ancestor",
          "birthYear": 1885,
          "deathYear": 1960
        }
      ]
    }
  ]
}
```

## Using AI to Generate Family Data

If you have a large amount of family data to organize, you can use AI to help you quickly generate JSON data in the correct format:

1. Prepare your family information text, including names, relationships, and relevant information for each generation.
2. Provide the following format guide to AI (such as DeepSeek, ChatGPT, Claude, etc.):

```
Please organize the family information I provide into the following JSON format:
{
  "generations": [
    {
      "title": "Xth Generation",
      "people": [
        {
          "id": "unique-identifier",
          "name": "Name",
          "info": "Detailed information",
          "fatherId": "Father's ID",
          "birthYear": birth year,
          "deathYear": death year
        }
      ]
    }
  | <details><summary>famil...</summary><p>familytree, nextjs, react, typescript</p></details> | 331 | https://github.com/qiaoshouqing/familytree |
| **[GoHomeEasy](https://github.com/kanshurichard/GoHomeEasy)** | GoHomeEasy æ˜¯ä¸€ä¸ªåŸºäº Cloudflare Workers çš„ Shadowsocks è®¢é˜…ç®¡ç†å·¥å…·ï¼Œä¸“ä¸º æ²¡æœ‰å…¬ç½‘ IP çš„å®¶åº­å®½å¸¦ç”¨æˆ· è®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨å¤–éƒ¨ç½‘ç»œè®¿é—®å®¶åº­å±€åŸŸç½‘ã€‚ | JavaScript | # ğŸš€ GoHomeEasy

## English | [ä¸­æ–‡](README_CN.md)

**GoHomeEasy** is a Shadowsocks subscription management tool based on Cloudflare Workers, designed specifically for **home broadband users without a public IP** to access their home network remotely.

It leverages **Lucky's NAT traversal** and automatic subscription updates, allowing users to **access their home Shadowsocks server from anywhere** without frequently changing dynamic IP addresses and ports manually.

---

## ğŸŒŸ **Features**

âœ… **Ideal for home broadband users without a public IP to access their home LAN remotely**  
âœ… **Supports Lucky Webhook for automatic Shadowsocks subscription updates**  
âœ… **Supports dynamic configuration of Shadowsocks `method` (encryption method) and `password`**  
âœ… **Based on Cloudflare Workers + KV, no need for a self-hosted server**  
âœ… **API Key authentication ensures data security**  
âœ… **Supports Cloudflare custom domain access to bypass `workers.dev` restrictions in Mainland China**  

---

## âš™ï¸ **Prerequisites**

To successfully deploy **GoHomeEasy**, prepare the following:

ğŸ”¹ **Linux home server or OpenWRT router**  
ğŸ”¹ **Shadowsocks server setup** (Recommended: [PassWall2 plugin](https://github.com/xiaorouji/openwrt-passwall2) for OpenWRT)  
ğŸ”¹ **Install [Lucky NAT Traversal](https://lucky666.cn)** and map the Shadowsocks server port to the public network  
ğŸ”¹ **Cloudflare account** (free account is sufficient for Workers deployment)  
ğŸ”¹ **Domain managed by Cloudflare DNS** (optional, for bypassing `workers.dev` restrictions in China)  
ğŸ”¹ **Shadowsocks-compatible client for mobile/PC** (e.g., Shadowrocket on iOS)  

---

## ğŸ’» **Shadowsocks Server Configuration**

Using PassWall2 as an example:

1. Navigate to the "Server" tab in PassWall2 and click "Add"
2. Configure as follows:
   - Enable: âœ… Checked
   - Name: Custom
   - Type: Sing-Box
   - Protocol: Shadowsocks
   - Listening Port: 8000 (or custom)
   - Password: Custom
   - Encryption: Recommended `chacha20-ietf-poly1305`
   - Allow LAN Access: âœ… Checked
   - Keep other settings default
3. Click **Save & Apply**, return to the main menu
4. Check "Enable" and click **Save & Apply**

---

## ğŸ›  **Cloudflare Workers Configuration**

### 1ï¸âƒ£ **Create a Workers Service**
1. Log in to **[Cloudflare Dashboard](https://dash.cloudflare.com/)**
2. Go to **Workers & Pages**, click **Create**
3. Select **"Start from template" â†’ "Hello world"**
4. Enter **Service Name** (e.g., `GoHomeEasy`), click **Deploy**

### 2ï¸âƒ£ **Edit Workers Code**
1. Open the newly created Worker, click **"< / >"** to edit the code
2. Delete the default code
3. Paste **`GoHomeEasy.js` code** from this repository
4. Modify `"your_secure_api_key"` in the source code and keep it safe
5. Click **Deploy**

### 3ï¸âƒ£ **Bind Cloudflare KV Storage**
1. Navigate to **Objects & Storage â†’ KV**
2. Click **+ Create**, name it `GoHomeEasy_KV`
3. Go to your **Worker** â†’ **Settings**
4. Click **Bindings â†’ + Add KV Namespace**
   - **Variable Name**: `KV_NAMESPACE`
   - **KV Namespace**: Select `GoHomeEasy_KV`
5. Click **Deploy**

---

## ğŸŒ **Use Cloudflare Custom Domain (Optional, only recommend for Mainland China Users)**

Follow these steps: [EdgeTunnel Issue #27](https://github.com/zizifn/edgetunnel/issues/27)

---

## ğŸ”— **Configure Lucky Webhook**

In **Lucky Webhook Settings**, enter the following:

### 1ï¸âƒ£ **Webhook URL (POST Request)**
- **Cloudflare Workers Native Domain**:
  ```
  https://your-worker-name.workers.dev/
  ```
- **Cloudflare Custom Domain**:
  ```
  https://gohome.yourdomain.com/
  ```

### 2ï¸âƒ£ **Request Headers**
```json
  Content-Type: application/json
  Authorization: Bearer your_secure_api_key
```

### 3ï¸âƒ£ **Request Body**
```json
{
  "ip": "#{ip}",
  "port": "#{port}",
  "method": "chacha20-ietf-poly1305",
  "password": "your_password"
}
```

---

## ğŸ“¥ **Client Subscription Configuration**

Using Shadowrocket ğŸš€ as an example:

### 1ï¸âƒ£ **Add Subscription URL**
1. Open **Shadowrocket**, tap `+`, select |  | 265 | https://github.com/kanshurichard/GoHomeEasy |
| **[3dgrut](https://github.com/nv-tlabs/3dgrut)** |  | Cuda | # 3D Gaussian Ray Tracing (3DGRT) and 3D Gaussian Unscented Transform (3DGUT)

This repository provides the official implementations of **3D Gaussian Ray Tracing (3DGRT)** and **3D Gaussian Unscented Transform (3DGUT)**. Unlike traditional methods that rely on splatting, 3DGRT performs ray tracing of volumetric Gaussian particles instead. This enables support for distorted cameras with complex, time-dependent effects such as rolling shutters, while also efficiently simulating secondary rays required for rendering phenomena like reflection, refraction, and shadows. However, 3DGRT requires dedicated ray-tracing hardware and remains slower than 3DGS.

To mitigate this limitation, we also propose 3DGUT, which enables support for distorted cameras with complex, time-dependent effects within a rasterization framework, maintaining the efficiency of rasterization methods. By aligning the rendering formulations of 3DGRT and 3DGUT, we introduce a hybrid approach called **3DGRUT**. This technique allows for rendering primary rays via rasterization and secondary rays via ray tracing, combining the strengths of both methods for improved performance and flexibility.

## Research Papers
- **3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes**  
  [Nicolas Moenne-Loccoz](https://www.linkedin.com/in/nicolas-moÃ«nne-loccoz-71040512/?original_referer=https%3A%2F%2Fwww%2Egoogle%2Ecom%2F&originalSubdomain=ca), [Ashkan Mirzaei](https://ashmrz.github.io), [Or Perel](https://orperel.github.io/), [Riccardo De Lutio](https://riccardodelutio.github.io/), [Janick Martinez Esturo](https://jme.pub/),   
  [Gavriel State](https://www.linkedin.com/in/gavstate/?originalSubdomain=ca), [Sanja Fidler](https://www.cs.utoronto.ca/~fidler/), [Nicholas Sharp](https://nmwsharp.com/), [Zan Gojcic](https://zgojcic.github.io/)  
  _SIGGRAPH Asia 2024 (Journal Track)_  
  **[Project page](https://research.nvidia.com/labs/toronto-ai/3DGRT)** / **[Paper](https://research.nvidia.com/labs/toronto-ai/3DGRT/res/3dgrt_compressed.pdf)** / **[Video](https://research.nvidia.com/labs/toronto-ai/3DGRT/res/3dgrt_supplementary_video.mp4)** / **[BibTeX](/-/raw/release/assets/3dgrt2024.bib)**

- **3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian Splatting**  
  [Qi Wu](https://wilsoncernwq.github.io/), [Janick Martinez Esturo](https://jme.pub/), [Ashkan Mirzaei](https://ashmrz.github.io),   
  [Nicolas Moenne-Loccoz](https://www.linkedin.com/in/nicolas-moÃ«nne-loccoz-71040512/?original_referer=https%3A%2F%2Fwww%2Egoogle%2Ecom%2F&originalSubdomain=ca), [Zan Gojcic](https://zgojcic.github.io/)  
  _CVPR 2025_  
  **[Project page](https://research.nvidia.com/labs/toronto-ai/3DGUT)** / **[Paper](https://research.nvidia.com/labs/toronto-ai/3DGUT/res/3DGUT_ready_main.pdf)** / **[Video](https://research.nvidia.com/labs/toronto-ai/3DGUT/#supp_video)** / **[BibTeX](/-/raw/release/assets/3dgut2025.bib)**

## ğŸ”¥ News
- âœ… 2025/03: Initial code release!
- âœ… 2025/02: [3DGUT](https://research.nvidia.com/labs/toronto-ai/3DGUT/res/3DGUT_ready_main.pdf) was accepted to CVPR 2025!
- âœ… 2024/08: [3DGRT](https://research.nvidia.com/labs/toronto-ai/3DGRT/res/3dgrt_compressed.pdf) was accepted to SIGGRAPH Asia 2024!

## Contents
- [ğŸ”¥ News](#-news)
- [Contents](#contents)
- [ğŸ”§ 1 Dependencies and Installation](#-1-dependencies-and-installation)
- [ğŸ’» 2. Train 3DGRT or 3DGUT scenes](#-2-train-3dgrt-or-3 |  | 263 | https://github.com/nv-tlabs/3dgrut |
| **[rf-detr](https://github.com/roboflow/rf-detr)** | RF-DETR is a real-time object detection model architecture developed by Roboflow, released under the Apache 2.0 license. | Python | # RF-DETR: SOTA Real-Time Object Detection Model

[![hf space](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/SkalskiP/RF-DETR)
[![colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-rf-detr-on-detection-dataset.ipynb)
[![roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/rf-detr)
[![discord](https://img.shields.io/discord/1159501506232451173?logo=discord&label=discord&labelColor=fff&color=5865f2&link=https%3A%2F%2Fdiscord.gg%2FGbfgXGJ8Bk)](https://discord.gg/GbfgXGJ8Bk)
[![version](https://badge.fury.io/py/rfdetr.svg)](https://badge.fury.io/py/rfdetr)
[![license](https://img.shields.io/pypi/l/rfdetr)](https://github.com/roboflow/rfdetr/blob/main/LICENSE)
[![python-version](https://img.shields.io/pypi/pyversions/rfdetr)](https://badge.fury.io/py/rfdetr)

RF-DETR is a real-time, transformer-based object detection model architecture developed by Roboflow and released under the Apache 2.0 license.

RF-DETR is the first real-time model to exceed 60 AP on the [Microsoft COCO benchmark](https://cocodataset.org/#home) alongside competitive performance at base sizes. It also achieves state-of-the-art performance on [RF100-VL](https://github.com/roboflow/rf100-vl), an object detection benchmark that measures model domain adaptability to real-world problems. RF-DETR is comparable in speed to current real-time object detection models.

**RF-DETR is small enough to run on the edge, making it an ideal model for deployments that need both strong accuracy and real-time performance.**

## Results

We validated the performance of RF-DETR on both Microsoft COCO and the RF100-VL benchmarks.

![rf-detr-coco-rf100-vl-8](https://media.roboflow.com/rf-detr/charts.png)

| Model            | mAP<sup>COCO val<br>@0.50:0.95 | mAP<sup>RF100-VL<br>Average @0.50 | Total Latency<br><sup>T4 bs=1<br>(ms) |
|:----------------:|:------------------------------:|:---------------------------------:|:-------------------------------------:|
| D-FINE-M         | **55.1**                       | N/A                               | 6.3                                   |
| LW-DETR-M        | 52.5                           | 84.0                              | <ins>6.0</ins>                        |
| YOLO11m          | 51.5                           | 84.9                              | **5.7**                               |
| YOLOv8m          | 50.6                           | <ins>85.0</ins>                   | 6.3                                   |
| RF-DETR-B        | <ins>53.3</ins>                | **86.7**                          | <ins>6.0</ins>                        |

<details>
<summary>Benchmark notes</summary>

The "Total Latency" reported here is measured on a T4 GPU using TensorRT10 FP16 (ms/img) and was introduced by LW-DETR. Unlike transformer-based models, YOLO models perform Non-Maximum Suppression (NMS) after generating predictions to refine bounding box candidates. While NMS boosts accuracy, it also slightly reduces speed due to the additional computation required, which varies with the number of objects in an image. Notably, many YOLO benchmarks include NMS in accuracy measurements but exclude it from speed metrics. By contrast, our benchmarkingâ€”following LW-DETRâ€™s approachâ€”factors in NMS latency to provide a uniform measure of the total time needed to obtain a final result across all models on the same hardware.

D-FINEâ€™s fine-tuning capability is currently unavailable, making its domain adaptability performance inaccessible. The authors [caution](https://github.com/Peterande/D-FINE) that â€œif your categories are very simple, it might lead to overfitting and suboptimal performance.â€ Furthermore, several open issues ([#108](https://github.com/Peterande/D-FINE/issues/108), [ | <details><summary>compu...</summary><p>computer-vision, detr, machine-learning, object-detection, rf-detr</p></details> | 256 | https://github.com/roboflow/rf-detr |
| **[vite-plugin-vue-mcp](https://github.com/webfansplz/vite-plugin-vue-mcp)** | Vite plugin that enables a MCP server helping models to understand your Vue app better. | Vue | # vite-plugin-vue-mcp

[![npm version][npm-version-src]][npm-version-href]
[![npm downloads][npm-downloads-src]][npm-downloads-href]
[![bundle][bundle-src]][bundle-href]
[![License][license-src]][license-href]

Vite plugin that enables a MCP (Model Context Protocol) server for your Vue app to provide information about the component tree, state, routes, and Pinia tree and state.

## Installation ğŸ“¦

To install the plugin, run the following command:

```bash
pnpm install vite-plugin-vue-mcp -D
```

## Usage ğŸ”¨

To use the plugin, you need to modify your `vite.config.ts` file as follows:

```ts
// vite.config.ts
import { defineConfig } from 'vite';
import { VueMcp } from 'vite-plugin-vue-mcp';

export default defineConfig({
  plugins: [VueMcp()],
});
```

Once configured, the MCP server will be available at `http://localhost:[port]/__mcp/sse`.

If you are using Cursor, create a `.cursor/mcp.json` file in your project root. This plugin will automatically update it for you. For more details about the MCP, check the [official Cursor documentation](https://docs.cursor.com/context/model-context-protocol).

### Options

You can customize the behavior of the plugin using the following options:

```ts
export interface VueMcpOptions {
  /**
   * The host to listen on, default is `localhost`
   */
  host?: string;

  /**
   * Print the MCP server URL in the console
   *
   * @default true
   */
  printUrl?: boolean;

  /**
   * The MCP server info. Ignored when `mcpServer` is provided
   */
  mcpServerInfo?: McpServerInfo;

  /**
   * Custom MCP server, when this is provided, the built-in MCP tools will be ignored
   */
  mcpServer?: (viteServer: ViteDevServer) => Awaitable<McpServer>;

  /**
   * Setup the MCP server, this is called when the MCP server is created
   * You may also return a new MCP server to replace the default one
   */
  mcpServerSetup?: (server: McpServer, viteServer: ViteDevServer) => Awaitable<void | McpServer>;

  /**
   * The path to the MCP server, default is `/__mcp`
   */
  mcpPath?: string;

  /**
   * Update the address of the MCP server in the cursor config file `.cursor/mcp.json`,
   * if `.cursor` folder exists.
   *
   * @default true
   */
  updateCursorMcpJson?: boolean | {
    enabled: boolean;
    /**
     * The name of the MCP server, default is `vue-mcp`
     */
    serverName?: string;
  };

  /**
   * Append an import to the module id ending with `appendTo` instead of adding a script into body
   * Useful for projects that do not use an HTML file as an entry
   *
   * WARNING: only set this if you know exactly what it does.
   * @default ''
   */
  appendTo?: string | RegExp;
}
```

## Features/Tools âœ¨

### Get Component Tree

- **Command**: `get-component-tree`
- **Description**: Get the Vue component tree.
  
![component-tree](./screenshots/component-tree.gif)

### Get Component State

- **Command**: `get-component-state`
- **Description**: Get the state of a component (input: `componentName`).

![component-state](./screenshots/component-state.gif)

### Edit Component State

- **Command**: `edit-component-state`
- **Description**: Edit the state of a component (input: `componentName`, `path`, `value`, `valueType`).

![edit-component-state](./screenshots/edit-component-state.gif)

### Highlight Component

- **Command**: `highlight-component`
- **Description**: Highlight a component (input: `componentName`).

![highlight-component](./screenshots/highlight-component.gif)

### Get Routes

- **Command**: `get-router-info`
- **Description**: Get the Vue router info of the application.

![route-tree](./screenshots/router-info.gif)

### Get Pinia Tree

- **Command**: `get-pinia-tree`
- **Description**: Get the Pinia tree of the application.

![pinia-tree](./screenshots/pinia-tree.gif)

### Get Pinia State

- **Command**: `get-pinia-state`
- **Description**: Get the Pinia state of the application (input: `storeName`).

![pinia-state](./screenshots/pinia-state.gif)

## Architecture âš¡ï¸

![architecture](./screenshots/architecture.png |  | 250 | https://github.com/webfansplz/vite-plugin-vue-mcp |
| **[bambot](https://github.com/timqian/bambot)** | Low cost (~$300) humanoid robot ğŸŒ± | TypeScript | It looks like you are sharing information about "Bambot," an open-source, low-cost humanoid robot that costs approximately $300. The project appears to be inspired by other robotic projects and provides links to its hardware and software documentation, as well as a demo video.

Hereâ€™s a summary of the information provided:

---

# Bambot

**Description**: Open source, low-cost humanoid robot (~$300). Inspired by:
- [lerobot](https://github.com/huggingface/lerobot)
- [so-100](https://github.com/TheRobotStudio/SO-ARM100)
- [lekiwi](https://github.com/SIGRobotics-UIUC/LeKiwi)

## Links
- **Hardware Documentation**: [View Hardware](./hardware)
- **Software Documentation**: [View Software](./software)

## Demo Video
[Watch Demo](https://x.com/Tim_Qian/status/1901952877243122014)

## Community Links
- [Join us on Discord](https://discord.gg/Fq2gvSMyRJ)
- [Follow us on WeChat](https://i.v2ex.co/1U6OSqswl.jpeg)
- [Follow Tim Qian on X](https://x.com/tim_qian)

---

If you need further assistance or have specific questions about the project, feel free to ask! |  | 246 | https://github.com/timqian/bambot |
| **[kiss-slam](https://github.com/PRBonn/kiss-slam)** | A LiDAR SLAM system that just works | Python | # KISS-SLAM

<div align="center">
    <a href="https://github.com/PRBonn/kiss-slam/releases"><img src="https://img.shields.io/github/v/release/PRBonn/kiss-slam?label=version" /></a>
    <a href="https://github.com/PRBonn/kiss-slam/blob/main/LICENSE"><img src="https://img.shields.io/github/license/PRBonn/kiss-slam" /></a>
    <a href="https://github.com/PRBonn/kiss-slam/blob/main/"><img src="https://img.shields.io/badge/Linux-FCC624?logo=linux&logoColor=black" /></a>
    <a href="https://github.com/PRBonn/kiss-slam/blob/main/"><img src="https://img.shields.io/badge/mac%20os-000000?&logo=apple&logoColor=white" /></a>
    <br />
    <br />
    <a href="https://github.com/PRBonn/kiss-slam/blob/main/README.md#Install">Install</a>
    <span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span>
    <a href="https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/kiss2025iros.pdf">Paper</a>
    <span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span>
    <a href="https://github.com/PRBonn/kiss-slam/issues">Contact Us</a>
    <br />
    <br />
    <p>KISS-SLAM is a simple, robust, and accurate 3D LiDAR SLAM system that <strong>just works</strong>.</p>
    <img src="https://github.com/user-attachments/assets/66c3e50f-009a-4a36-9856-283a895c300f" alt="motivation" />
</div>

<hr />

## Install

To install KISS-SLAM, run the following command:

```bash
pip install kiss-slam
```

## Running the System

To run the system and see the available commands, type:

```bash
kiss_slam_pipeline --help
```

This will display a help message to guide you.

### Config

You can generate a default configuration file by executing:

```bash
kiss_slam_dump_config
```

This will create a `kiss_slam.yaml` file that you can modify. Pass this file to the `--config` option when running the `kiss_slam_pipeline`.

### Install Python API (Developer Mode)

For development purposes, you can install the Python API in developer mode with the following commands:

```bash
sudo apt install git python3-pip libeigen3-dev libsuitesparse-dev
git clone https://github.com/PRBonn/kiss-slam.git
cd kiss-slam
make editable
```

## Citation

If you use this library for academic work, please cite our original paper:

```bib
@article{kiss2025arxiv,
  author   = {T. Guadagnino and B. Mersch and S. Gupta and I. Vizzo and G. Grisetti and C. Stachniss},
  title    = {{KISS-SLAM: A Simple, Robust, and Accurate 3D LiDAR SLAM System With Enhanced Generalization Capabilities}},
  journal  = {arXiv preprint},
  year     = 2025,
  volume   = {arXiv:2503.12660},
  url      = {https://arxiv.org/pdf/2503.12660},
}
```

## Acknowledgements

This project builds on top of:
- [KISS-ICP](https://github.com/PRBonn/kiss-icp)
- [MapClosures](https://github.com/PRBonn/MapClosures)
- [g2o](https://github.com/RainerKuemmerle/g2o)

## Contributing

We envision KISS-SLAM as a community-driven project. We appreciate contributions from the community and welcome you to open a Pull Request!

<div align="center">
    <a href="https://github.com/PRBonn/kiss-slam/graphs/contributors">
        <img src="https://contrib.rocks/image?repo=PRBonn/kiss-slam" />
    </a>
</div>

## Contact Us

For questions or feedback, please reach out via:
- GitHub Issues: [KISS-SLAM Issues](https://github.com/PRBonn/kiss-slam/issues) | <details><summary>lidar...</summary><p>lidar, lidar-slam, mapping, perception, robotics, slam</p></details> | 235 | https://github.com/PRBonn/kiss-slam |
| **[fetcher-mcp](https://github.com/jae-jae/fetcher-mcp)** | MCP server for fetch web page content using Playwright headless browser. | TypeScript | # Fetcher MCP

Fetcher MCP is a server designed for fetching web page content using the Playwright headless browser. It is particularly useful for retrieving dynamic content from modern web applications.

## Advantages

- **JavaScript Support**: Unlike traditional web scrapers, Fetcher MCP can execute JavaScript, allowing it to handle dynamic web content effectively.
  
- **Intelligent Content Extraction**: The built-in Readability algorithm extracts the main content from web pages, filtering out ads and other non-essential elements.

- **Flexible Output Format**: Outputs can be in HTML or Markdown, facilitating easy integration with various applications.

- **Parallel Processing**: The `fetch_urls` tool allows for concurrent fetching of multiple URLs, enhancing efficiency.

- **Resource Optimization**: Automatically blocks unnecessary resources (like images and stylesheets) to save bandwidth and improve performance.

- **Robust Error Handling**: Comprehensive error handling and logging ensure reliable operations even with problematic web pages.

- **Configurable Parameters**: Offers fine control over timeouts, content extraction, and output formatting to cater to different use cases.

## Quick Start

To run Fetcher MCP directly with npx:

```bash
npx -y fetcher-mcp
```

### Debug Mode

To run in debug mode (showing the browser window for debugging):

```bash
npx -y fetcher-mcp --debug
```

## Configuration MCP

To configure the MCP server in Claude Desktop, modify the configuration file located at:

- On MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`
- On Windows: `%APPDATA%/Claude/claude_desktop_config.json`

Example configuration:

```json
{
  "mcpServers": {
    "fetcher": {
      "command": "npx",
      "args": ["-y", "fetcher-mcp"]
    }
  }
}
```

## Features

### `fetch_url`

- Retrieve web page content from a specified URL using the Playwright headless browser.
- Supports intelligent extraction and conversion to Markdown.
- **Parameters**:
  - `url`: Required URL of the web page.
  - `timeout`: Page loading timeout in milliseconds (default: 30000).
  - `waitUntil`: Navigation completion criteria (options: 'load', 'domcontentloaded', 'networkidle', 'commit'; default: 'load').
  - `extractContent`: Whether to extract main content (default: true).
  - `maxLength`: Maximum length of returned content (default: no limit).
  - `returnHtml`: Return HTML instead of Markdown (default: false).
  - `waitForNavigation`: Wait for additional navigation (default: false).
  - `navigationTimeout`: Maximum wait time for additional navigation (default: 10000).
  - `disableMedia`: Disable media resources (default: true).
  - `debug`: Enable debug mode for this request (overrides the command-line flag).

### `fetch_urls`

- Batch retrieve web page content from multiple URLs in parallel.
- Uses multi-tab fetching for improved performance.
- **Parameters**:
  - `urls`: Required array of URLs to fetch.
  - Other parameters are the same as `fetch_url`.

## Tips

### Handling Special Website Scenarios

#### Dealing with Anti-Crawler Mechanisms

- **Complete Loading**: For CAPTCHA or redirects, prompt with:
  ```
  Please wait for the page to fully load
  ```
  
- **Increase Timeout**: For slow-loading sites:
  ```
  Please set the page loading timeout to 60 seconds
  ```

#### Content Retrieval Adjustments

- **Preserve Original HTML**: To avoid extraction failure:
  ```
  Please preserve the original HTML content
  ```

- **Fetch Complete Content**: For limited extracted content:
  ```
  Please fetch the complete webpage content instead of just the main content
  ```

- **Return HTML**: To get content in HTML format:
  ```
  Please return the content in HTML format
  ```

### Debugging and Authentication

#### Enabling Debug Mode

- **Dynamic Debug Activation**:
  ```
  Please enable debug mode for this fetch operation
  ```

#### Using Custom Cookies for Authentication

- **Manual Login**: To log in with credentials:
  ```
  Please run in debug mode so I can manually log in to the website
  ```

- **Interacting with Debug Browser**: In debug mode, you can log in manually after the browser opens.

## Development

### Install Dependencies

```bash
npm install
```

### Install Playwright Browser

```bash
npm run install-browser
```

### Build the Server

```bash
npm run build
```

## Debugging

Use MCP Inspector for debugging:

```bash
npm run inspector
```

To enable visible browser mode for debugging:

```bash
node build/index.js --debug
```

## License

Licensed under the [MIT License](https://choosealicense | <details><summary>ai, m...</summary><p>ai, mcp, playwright</p></details> | 231 | https://github.com/jae-jae/fetcher-mcp |

